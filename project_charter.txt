PROJECT DESIGN — SMART goals

S:Specific — Experiment with fine-tuning a reasoning model with Unsloth using Group Relative Policy Optimization (GRPO), a Reinforcement Learning algorithm for improving the model's reasoning ability. Are LLMs genuinely capable of true logical reasoning?

M:Measurable — Track progress by evaluating on GSM8k benchmark to assess the mathematical reasoning of models based on accuracy, agreement (step consistency and evaluated expression), and failure rate (llm outputing invalid responses). Other performance/efficiency attributes include the throughput (number of inferences processed per second) and latency (time taken to generate a prediction). 

A:Achievable — Develop baseline model (preferably 2 models) to assess performance relative to SOTA reasoning models and iteratively store/evaluate results for evaluation. The actual implementation is achievable per Unsloth's API.

R:Relevant — Fine-tuning language models for [better] reasoning capabilities enables these models to generate consistent and possibly true outputs, and allows for their interpretability. 

T:Time-bound — The deadline for the project is May 9th (code & report).


TEAM COMPOSITION — duties

Abhay Nandiraju: 
- Code: Conduct baseline, train model with GRPO, design reward functions.
- Paper:  GRPO math,
Matthew Hernandez:
- Code: Latency experiments, repository maintainer, inference notebook.
- Paper: GSM8K dataset, operational metrics, 
Ayesha Khatun:
- Paper: Introduction, related works, experiment results (error analysis)
Maksim Kulik:
- Paper: Gemma-3 1B, methodology, discussion and conclusion

RELATED WORK
Perturbation experiment: https://arxiv.org/pdf/2410.05229
GSM8K: https://arxiv.org/pdf/2110.14168

USEFUL LINKS
https://symbl.ai/developers/blog/a-guide-to-llm-inference-performance-monitoring/
https://www.proxet.com/blog/llm-has-a-performance-problem-inherent-to-its-architecture-latency


AI/ML ENGINEERING PRINCIPLES 
Frameworks: Unsloth AI
Infrastructure: Cloud-native
Constraints: Latency (llm response time) & training time, interpretability, GPU required
Business Objectives: Experimentation
Fairness: N/A for reasoning
Model Compression Readiness: 4bit model

1. Core Software Engineering Attributes
Deployability: Ease of deploying/integration via Hugging Face
Observability: Analyze system performance automatically with trainer
Testability: Perturbation experiment

2. ML-Specific Model Attributes
Interpretability: how well the model's decisions can be understood
	- step-by-step coherence & evaluate expression == answer
Generalizability: model's performance on unseen, real-world data
	- zero-shot, few-shot evaluation on baseline/final model
	- multiple runs per epoch/baseline (3-5) avg & report mean ± std. 
Robustness: ability to handle noisy, incomplete, or adverarial data
	- Add a single clause (appears relevant) and observe performance to measure reasoning vs. memorization 
	- Measure sensitivity by changing numbers and names

3. Performance & Efficiency Attributes
Efficiency: Optimal use of T4 GPU via Google Colab
Throughput: Number of inferences processed per second—not necessary, unless business use.
Latency: Time taken for the model to generate a prediction (i.e., ...)—useful.
