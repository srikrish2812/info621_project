{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOiu4aGhsNdx582Aqb+Gh2g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srikrish2812/info621_project/blob/mah_issn/src/notebooks/gemma_3_final_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemma 3 — Inference"
      ],
      "metadata": {
        "id": "7cXV7sA7Bsyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "insert Google Colab link"
      ],
      "metadata": {
        "id": "qCZ_6MX2YIHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Framework/ Library Installation"
      ],
      "metadata": {
        "id": "-nMAoD0RBzao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Installs unsloth and other dependencies optimized for colab\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --upgrade transformers accelerate bitsandbytes datasets asteval GPUtil"
      ],
      "metadata": {
        "id": "UeQy_ccMBuFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os\n",
        "import re\n",
        "import pdb\n",
        "\n",
        "import unsloth\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "xKXYXTclCnbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use helper functions from repository\n",
        "\n",
        "colab = True\n",
        "if colab:\n",
        "  if not os.path.exists(\"info621/\"):\n",
        "    !git clone https://github.com/srikrish2812/info621_project info621"
      ],
      "metadata": {
        "id": "ce-dgHZgZZWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the final model and dataset"
      ],
      "metadata": {
        "id": "SVzn3GuWMYEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model():\n",
        "  \"\"\"\n",
        "  Load the trained Gemma 3 1B with Unsloth.\n",
        "  \"\"\"\n",
        "  model_name = \"abhay2812/gemma-3-1b-4bit-grpo\"\n",
        "  model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "      model_name,\n",
        "      load_in_4bit=True,\n",
        "      device_map=\"auto\"\n",
        "  )\n",
        "  FastLanguageModel.for_inference(model)\n",
        "\n",
        "  return model, tokenizer\n",
        "\n",
        "model, tokenizer = load_model()"
      ],
      "metadata": {
        "id": "MfkCjYtzMr6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from info621.src.tasks.gsm8k import GSM8kTask\n",
        "\n",
        "gsm8k = GSM8kTask()\n",
        "dataset = gsm8k.get_questions()"
      ],
      "metadata": {
        "id": "iiOWsOWnaeZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "- Run inference on final model"
      ],
      "metadata": {
        "id": "Jf4xl6RXYdaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer"
      ],
      "metadata": {
        "id": "NrREca3hNNet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_sample = gsm8k.__getsamples__(n_samples=1, split=\"test\")\n",
        "random_sample['prompt']"
      ],
      "metadata": {
        "id": "nwdWr7msqNCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model(sample, measure_p=False):\n",
        "  text = tokenizer.apply_chat_template(\n",
        "      sample,\n",
        "      add_generation_prompt = True,\n",
        "      tokenize = False,\n",
        "  )\n",
        "  streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "  if measure_p:\n",
        "    streamer = None\n",
        "\n",
        "  tensor = model.generate(\n",
        "      **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "      max_new_tokens = 128,\n",
        "      temperature = 1.0, top_p = 0.95, top_k = 64,\n",
        "      streamer = streamer,\n",
        "  )\n",
        "  return tensor"
      ],
      "metadata": {
        "id": "G4SdR2fRyA5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_output = run_model(random_sample['prompt'])"
      ],
      "metadata": {
        "id": "Q5jbbsjKEOEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_output = tokenizer.decode(model_output[0])\n",
        "y = extract_final_number(decoded_output)\n",
        "random_sample['answer'][0] == y"
      ],
      "metadata": {
        "id": "dB8fnsmMradH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How Fragile is Mathematical Reasoning in Large Language Models?\n",
        "\n"
      ],
      "metadata": {
        "id": "WMD_7AUzIQRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets"
      ],
      "metadata": {
        "id": "3NQvJ5DVKwiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples_names = gsm8k.__getsamples__(n_samples=100, split=\"test\")\n",
        "samples_numbers = gsm8k.__getsamples__(n_samples=100, split=\"test\")\n",
        "\n",
        "ds_pert = datasets.DatasetDict(\n",
        "    {\"names\": samples_names,\n",
        "     \"numbers\": samples_numbers})"
      ],
      "metadata": {
        "id": "CPweKeO4IfKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_perturbation_experiment(dataset_dict):\n",
        "  experiment = {}\n",
        "  for config in dataset_dict:\n",
        "    experiment[f\"{config}\"] = []\n",
        "    for sample in test:\n",
        "      model_output = run_model(sample)\n",
        "      experiment[config].append(model_output)\n",
        "\n",
        "  return experiment"
      ],
      "metadata": {
        "id": "NSmWTvoPJSHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# experiment_output = run_perturbation_experiment(ds_pert)"
      ],
      "metadata": {
        "id": "z4sBtlq5MnHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_perturbation_experiment(experiment_output):\n",
        "  #TODO compute results from experiment\n",
        "  pass"
      ],
      "metadata": {
        "id": "XQUsAFvyMfJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Measure Gemma 3 Performance — Latency & Throughput on GPU"
      ],
      "metadata": {
        "id": "j5Ehi981vsw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import GPUtil\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "96CIycNAGpZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Measure latency (i.e., amount of time it takes the model to produce a prediction for a single input sample) for *n* iterations."
      ],
      "metadata": {
        "id": "9TVieaVd2NfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_latency(sample, iterations=1):\n",
        "  latencies = []\n",
        "  gpus = GPUtil.getGPUs()\n",
        "\n",
        "  if not gpus:\n",
        "    raise ValueError(\"No GPUs found\")\n",
        "  gpu = gpus[0]\n",
        "\n",
        "  for _ in range(iterations):\n",
        "    start_time = time.time()\n",
        "    run_model(sample['prompt'], measure_p=True)\n",
        "    end_time = time.time()\n",
        "    latencies.append(end_time-start_time)\n",
        "\n",
        "  return latencies"
      ],
      "metadata": {
        "id": "lpp3577HwZBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterations = 10\n",
        "latencies = test_latency(random_sample, iterations=iterations)"
      ],
      "metadata": {
        "id": "yznVFvCNxOfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Average latencies per {iterations} iterations: {np.mean(latencies):.4f} seconds\")\n",
        "print(f\"Maximum latency per {iterations} iterations: {np.max(latencies):.4f} seconds\")\n",
        "print(f\"Minimum latency per {iterations} iterations: {np.min(latencies):.4f} seconds\")"
      ],
      "metadata": {
        "id": "8fZW48i-1Dtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "About 7 seconds per prediction, expensive?"
      ],
      "metadata": {
        "id": "j4a-hizS3ezG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Measure throughput (i.e., number of predictions) given *n* amount of time."
      ],
      "metadata": {
        "id": "LD1kSDp22Vf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def benchmark_throughput(sample, runs=5):\n",
        "  #TODO\n",
        "  \"\"\"Can do batch size = 4, if time permits. 256 generation...\"\"\"\n",
        "  for _ in range(3):\n",
        "    run_model(sample, measure_p=True)\n",
        "  torch.cuda.synchronize()\n",
        "\n",
        "  total_gen_tokens=0\n",
        "\n",
        "  start_time = time.time()\n",
        "  for _ in range(runs):\n",
        "    input_len = len(tokenizer.apply_chat_template(\n",
        "        random_sample['prompt'],\n",
        "        add_generation_prompt = True,\n",
        "        tokenize = True)[0])\n",
        "    torch.cuda.synchronize()\n",
        "    output_tensor = run_model(sample, measure_p=True)\n",
        "    total_gen_tokens += output_tensor.shape[-1] - input_len\n",
        "  end_time = time.time()\n",
        "  elapsed = end_time - start_time\n",
        "  throughput = total_gen_tokens / elapsed\n",
        "\n",
        "  return throughput, runs"
      ],
      "metadata": {
        "id": "0I68Ov9O4hLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "throughput, runs = benchmark_throughput(sample=random_sample[\"prompt\"])"
      ],
      "metadata": {
        "id": "O3i-UJ4L4rXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Throughput: {throughput:.2f} tokens/sec over {runs} runs\")"
      ],
      "metadata": {
        "id": "E0rtwyaxHGw_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}