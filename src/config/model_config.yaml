# model_config.yaml

config:
  project: grpo-baselines
  model: unsloth/gemma-3-1b-it-bnb-4bit
  dataset: GSM8K

training: #PENDING
  seed: none
  torch_deterministic: false
  device: cuda
  load_in4bit: true
  evaluation: exact-match
  training_batch_size: 128
  temperature: 0.0
  dataset_split: train
  max_generation_tokens: 200

inference: #PENDING & DOUBLE-CHECK
  seed: none
  torch_deterministic: false
  device: cuda
  load_in4bit: true
  evaluation: exact-match
  evaluation_batch_size: 128
  temperature: 0.0
  dataset_split: test
  max_generation_tokens: 200

tokenizer:
  padding: true
  padding_side: left
  pad_token: eos_token
  truncation: true
  max_length: 1024

prompt_format:
  prompt: <start_of_turn>user\nSolve the following math problem step-by-step:\n{question}<end_of_turn>\n<start_of_turn>model\n
  zero_shot: true